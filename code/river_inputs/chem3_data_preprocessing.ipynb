{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a0c797-f26f-49f7-a800-90d9ebf7bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import math\n",
    "import uuid\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3ba84-10ad-4cc1-87ca-de140bfa59fe",
   "metadata": {},
   "source": [
    "#  Harmonize and Export River Water Chemistry Data\n",
    "\n",
    "This notebook performs reconstruction, calculation, quality control, and export of river water chemistry data derived from NIVA’s database. It processes NetCDF datasets, computes derived parameters, filters outliers, attaches CF-compliant metadata, and exports the results as standardized NetCDF files.\n",
    "\n",
    "### How to run the analysis?\n",
    "1. Clone the AquaINFRA GitHub repo\n",
    "2. Make sure the following dependencies are installed in the environment: xarray, pandas, numpy, matplotlib, pathlib\n",
    "3. Define input files, metadata, and output files.\n",
    "4. Run all cells sequentially.\n",
    "\n",
    "### Which programming languages are used?\n",
    "- Python\n",
    "\n",
    "### How does the workflow look like?\n",
    "1. Read raw NetCDF files\n",
    "2. Convert to pandas DataFrames\n",
    "3. Reconstruct missing values\n",
    "4. Compute derived parameters\n",
    "5. Visualize original vs derived variables\n",
    "6. Apply outlier filtering using percentile thresholds\n",
    "7. Convert to xarray.Dataset and assign time + station coordinates\n",
    "8. Attach variable and global metadata\n",
    "9. Export NetCDF files \n",
    "\n",
    "### Which functions are used and how are they connected?\n",
    "- `plot_reconstruction()` – Plots original vs reconstructed variables\n",
    "- `plot_scatter()` – Compares calculated vs observed values\n",
    "- `plot_lines()` – Plots time series\n",
    "- `detect_outliers_per_station()` – Identifies and replaces outliers using 0.05–99 percentile filtering\n",
    "- `xr.Dataset.from_dataframe()` – Converts cleaned DataFrames to xarray.Dataset\n",
    "- `to_netcdf()` – Saves final datasets with CF/ACDD metadata to NetCDF format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8625c-4081-43af-8940-d54a8e7bd3c0",
   "metadata": {},
   "source": [
    "## 0. User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d6ed23-83c8-4e39-ab79-9e496a06f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input files \n",
    "wc_raw_data_dir = Path(\"../../data/river/water_chemistry/raw\")\n",
    "wc_raw_data_file_paths = list(wc_raw_data_dir.glob(\"raw_riverchem_*.nc\"))\n",
    "\n",
    "# Define parameters to reconstruct\n",
    "reconstruction_config = {\n",
    "    \"TOTN\": {\n",
    "        \"fallback\": \"TOTN_EF_usikker\",\n",
    "        \"units\": \"µg/l\",\n",
    "        \"preserve_as\": \"TOTN_orig\"\n",
    "    },\n",
    "    \"SiO2\": {\n",
    "        \"compute_from\": \"Si\",\n",
    "        \"formula\": \"Si * 2.14\",\n",
    "        \"units\": \"mg/l\",\n",
    "        \"preserve_as\": \"SiO2_orig\",\n",
    "        \"calc_temp_var\": \"SiO2_calc\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define derived parameters\n",
    "derivation_config = [\n",
    "    {\n",
    "        \"operation\": \"scale\",\n",
    "        \"target\": \"POC\",\n",
    "        \"factor\": 1 / 1000\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"sum\",\n",
    "        \"target\": \"TOC_calc\",\n",
    "        \"sources\": [\"POC\", \"DOC\"]\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"fill_from_other\",\n",
    "        \"target\": \"TDP\",\n",
    "        \"expression\": \"TOTP - PP\",\n",
    "        \"condition\": \"TDP.isna() & PP.notna()\",\n",
    "        \"requires\": [\"TDP\", \"PP\", \"TOTP\"]\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"fill_from_other\",\n",
    "        \"target\": \"PP\",\n",
    "        \"expression\": \"TOTP - TDP\",\n",
    "        \"condition\": \"PP.isna() & TDP.notna()\",\n",
    "        \"requires\": [\"TDP\", \"PP\", \"TOTP\"]\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"rowwise_sum\",\n",
    "        \"target\": \"DIN\",\n",
    "        \"sources\": [\"NO3-N\", \"NH4-N\"],\n",
    "        \"only_when_all_present\": True\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"difference\",\n",
    "        \"target\": \"DON\",\n",
    "        \"expression\": \"TOTN - DIN\",\n",
    "        \"requires\": [\"TOTN\", \"DIN\"]\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"mask_date_before\",\n",
    "        \"target\": \"TOC\",\n",
    "        \"file_contains\": \"40355\",\n",
    "        \"date_column\": \"sample_date\",\n",
    "        \"before\": \"1998-01-01\"\n",
    "    }, \n",
    "    {\n",
    "        \"operation\": \"mask_date_before\",\n",
    "        \"target\": \"DOC\",\n",
    "        \"file_contains\": \"40355\",\n",
    "        \"date_column\": \"sample_date\",\n",
    "        \"before\": \"2016-01-01\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"mask_date_before\",\n",
    "        \"target\": \"DOC\",\n",
    "        \"file_contains\": \"40352\",\n",
    "        \"date_column\": \"sample_date\",\n",
    "        \"before\": \"2016-01-01\"\n",
    "    }, \n",
    "    {\n",
    "        \"operation\": \"mask_date_before\",\n",
    "        \"target\": \"DOC\",\n",
    "        \"file_contains\": \"40356\",\n",
    "        \"date_column\": \"sample_date\",\n",
    "        \"before\": \"2016-01-01\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define variables to plot \n",
    "plot_config = [\n",
    "    {\n",
    "        \"type\": \"scatter\",\n",
    "        \"x\": \"TOC\",\n",
    "        \"y\": \"TOC_calc\",\n",
    "        \"xlabel\": \"TOC\",\n",
    "        \"ylabel\": \"TOC_calc\",\n",
    "        \"title\": \"TOC vs TOC_calc\",\n",
    "        \"unit\": \"mg/l\",\n",
    "        \"required\": [\"TOC\", \"TOC_calc\"],\n",
    "        \"subplot\": [0, 0]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"line\",\n",
    "        \"columns\": [\"TOC\", \"DOC\", \"POC\", \"TOC_calc\"],\n",
    "        \"labels\": [\"TOC\", \"DOC\", \"POC\", \"TOC_calc\"],\n",
    "        \"title\": \"TOC, DOC, POC, TOC_calc\",\n",
    "        \"unit\": \"mg/l\",\n",
    "        \"required\": [\"TOC\", \"DOC\", \"POC\", \"TOC_calc\"],\n",
    "        \"subplot\": [0, 1]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"line\",\n",
    "        \"columns\": [\"TDP\", \"PP\", \"TOTP\"],\n",
    "        \"labels\": [\"TDP\", \"PP\", \"TOTP\"],\n",
    "        \"title\": \"TDP, PP, TOTP\",\n",
    "        \"unit\": \"µg/l\",\n",
    "        \"required\": [\"TDP\", \"PP\", \"TOTP\"],\n",
    "        \"subplot\": [1, 0]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"line\",\n",
    "        \"columns\": [\"NO3-N\", \"NH4-N\", \"DIN\"],\n",
    "        \"labels\": [\"NO3\", \"NH4-N\", \"DIN\"],\n",
    "        \"title\": \"NO3, NH4-N, DIN\",\n",
    "        \"unit\": \"µg/l\",\n",
    "        \"required\": [\"NO3-N\", \"NH4-N\", \"DIN\"],\n",
    "        \"subplot\": [1, 1]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"line\",\n",
    "        \"columns\": [\"TOTN\", \"DIN\", \"DON\"],\n",
    "        \"labels\": [\"TOTN\", \"DIN\", \"DON\"],\n",
    "        \"title\": \"TOTN, DIN, DON\",\n",
    "        \"unit\": \"µg/l\",\n",
    "        \"required\": [\"TOTN\", \"DIN\", \"DON\"],\n",
    "        \"subplot\": [2, 0]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "#\n",
    "outlier_config = {\n",
    "    \"lower_quantile\": 0.0005,\n",
    "    \"upper_quantile\": 0.99\n",
    "}\n",
    "\n",
    "# Parameters metadata\n",
    "pars_metadata_df = pd.DataFrame({\n",
    "    \"parameter_name\": [\"DOC\", \"Color\", \"NH4-N\", \"NO3-N\", \"TRP\", \"POC\", \"POC_calc\", \"TOC_calc\", \"SPM\", \"SiO2\", \"TOC\", \"TOTN\", \"TOTP\", \"TDP\", \"DIN\", \n",
    "                       \"DON\", \"PP\", \"TSM\", \"UV_Abs_254nm\", \"UV_Abs_410nm\"],\n",
    "    \"unit\":  [\"mg/l C\", \"mg Pt/l\", \"µg/l\", \"µg/l\", \"µg/l\", \"mg/l\", \"mg/l\", \"mg/l\", \"mg/l\", \"mg/l\", \"mg/l\", \"µg/l\", \"µg/l\", \"µg/l P\", \"µg/l\", \n",
    "              \"µg/l\", \"µg/l P\", \"mg/l\", \"Abs/cm\", \"Abs/cm\"]\n",
    "})\n",
    "\n",
    "# Define a mapping from parameter names to long names\n",
    "standard_name_map = {\n",
    "    \"DOC\": \"Dissolved Organic Carbon\",\n",
    "    \"Color\": \"Water Color\",\n",
    "    \"NH4-N\": \"Ammonium-N\",\n",
    "    \"NO3-N\": \"Nitrate-N\",\n",
    "    \"TRP\": \"Total Reactive Phosphorus\",\n",
    "    \"POC\": \"Particulate Organic Carbon\",\n",
    "    \"TOC_calc\": \"Total Organic Carbon Calculated\",\n",
    "    \"SPM\": \"Suspended Particulate Matter\", \n",
    "    \"SiO2\": \"Silicate\", \n",
    "    \"TOC\": \"Total Organic Carbon\",\n",
    "    \"TOTN\": \"Total Nitrogen\",\n",
    "    \"TOTP\": \"Total Phosphorus\",\n",
    "    \"TDP\": \"Total Dissolved Phosphorus\",\n",
    "    \"DIN\": \"Dissolved Inorganic Nitrogen\",\n",
    "    \"DON\": \"Dissolved Organic Nitrogen\",\n",
    "    \"PP\": \"Particulate Phosphorus\",\n",
    "    \"TSM\": \"Total Suspended Matter\",\n",
    "    \"UV_Abs_254nm\": \"Ultraviolet Absorbance 254nm\",\n",
    "    \"UV_Abs_410nm\": \"Ultraviolet Absorbance 410nm\",\n",
    "}\n",
    "\n",
    "# Comments for selected variables\n",
    "var_comments = {\n",
    "    \"TOTN\": \"TOTN is reconstructed using Eurofins data (TOTN_EF_usikker), which are considered more uncertain than NIVA data. However, due to gaps in NIVA data from 2017–2021, Eurofins measurements were included to ensure completeness.\",\n",
    "    \"SiO2\": \"SiO2 is reconstructed from Si using a conversion factor of 2.14 (based on molecular weights). Different methods were used (photometric vs ICP-MS), but results are expected to be comparable.\",\n",
    "    \"TOC_calc\": \"TOC_calc is computed as POC + DOC, for comparison with lab-reported TOC. It is often more accurate due to rounding errors and turbidity effects in TOC analysis.\",\n",
    "    \"DIN\": \"DIN is calculated as NO3-N + NH4-N, only when both values are available.\",\n",
    "    \"DON\": \"DON is calculated as TOTN - DIN, only where both TOTN and DIN are available.\",\n",
    "    \"PP\": \"PP is computed as TOTP - TDP for the period 2017–2020.\",\n",
    "    \"TDP\": \"TDP is computed as TOTP - PP for the period 2021–2024.\",\n",
    "    \"TRP\": \"TRP includes both Soluble Reactive Phosphorus (SRP) and Particulate Reactive Phosphorus (PRP) fractions.\"\n",
    "}\n",
    "\n",
    "global_metadata_config = {\n",
    "    \"naming_authority\": \"no.niva\",\n",
    "    \"project\": \"AquaINFRA\",\n",
    "    \"iso_topic_category\": \"inlandWaters\",\n",
    "    \"featureType\": \"timeSeries\",\n",
    "    \"spatial_representation\": \"point\",\n",
    "    \"creator_type\": \"institution\",\n",
    "    \"creator_institution\": \"Norwegian Institute for Water Research (NIVA)\",\n",
    "    \"institution\": \"Norwegian Institute for Water Research (NIVA)\",\n",
    "    \"institution_short_name\": \"NIVA\",\n",
    "    \"creator_name\": \"Areti Balkoni\",\n",
    "    \"creator_email\": \"areti.balkoni@niva.no\",\n",
    "    \"creator_url\": \"https://www.niva.no/en/employees/areti-balkoni\",\n",
    "    \"data_owner\": \"Norwegian Institute for Water Research\",\n",
    "    \"processing_level\": \"Harmonized and cleaned\",\n",
    "    \"Conventions\": \"CF-1.7, ACDD-1.3\",\n",
    "    \"publisher_name\": \"Norwegian Institute for Water Research\",\n",
    "    \"publisher_email\": \"miljoinformatikk@niva.no\",\n",
    "    \"publisher_institution\": \"Norwegian Institute for Water Research\",\n",
    "    \"publisher_url\": \"https://niva.no\",\n",
    "    \"license\": \"http://spdx.org/licenses/CC-BY-4.0(CC-BY-4.0)\",\n",
    "    \"keywords\": \"GCMDSK:EARTH SCIENCE > WATER QUALITY > CHEMISTRY, GCMDLOC:CONTINENT > EUROPE > NORWAY\",\n",
    "    \"keywords_vocabulary\": \"GCMDSK:GCMD Science Keywords, GCMDLOC:GCMD Locations\",\n",
    "    \"history\": \"Harmonized and cleaned using percentile-based filtering\",\n",
    "    \"toc_calc_notes\": (\n",
    "        \"Lab-reported TOC values may underestimate total organic carbon in turbid samples due to the lack of stirring in the autosampler, \"\n",
    "        \"causing particles to settle. \"\n",
    "        \"TOC and DOC are rounded to 0.1 mg/L, adding further uncertainty, while POC is reported to the nearest µg/L. \"\n",
    "        \"Therefore, TOC_calc (POC + DOC) is often more reliable than lab-reported TOC, but all results should be interpreted considering analytical uncertainties.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "processed_namespace_uuid = uuid.UUID(\"a23b7946-1a42-4d4a-bb0d-cf5a6cfb5670\") \n",
    "\n",
    "# Where to save \n",
    "fig_dir = Path(\"../../figures/quality_control\")\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_dir = Path(\"../../data/river/water_chemistry/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f49f6-3989-481a-91e9-dd00302df8b0",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f439ce75-dd59-46fb-bf70-17059a0e944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dataframes = []\n",
    "for fp in wc_raw_data_file_paths:\n",
    "    ds = xr.open_dataset(fp)\n",
    "    df = ds.to_dataframe().reset_index() # Convert to dataframes for easier analysis\n",
    "    \n",
    "    # Make sure date is datetime\n",
    "    if \"sample_date\" in df.columns:\n",
    "        df[\"sample_date\"] = pd.to_datetime(df[\"sample_date\"], errors=\"coerce\")\n",
    "    elif \"time\" in df.columns:\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "\n",
    "    initial_dataframes.append((fp.name, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de6c54-ff9e-4cb9-9c51-1adef87e915a",
   "metadata": {},
   "source": [
    "## 2. Reconstruct TOTN and SiO2 time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0971c3dc-849d-45fd-944d-666cbb11752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(df, date_col, var_orig, var_final,\n",
    "                        label_base=\"\", station_label=\"\", units=\"\", output_path=None, colors=None):\n",
    "    colors = colors or {'original': '#1b9e77', 'final': '#d95f02'}\n",
    "\n",
    "    if var_orig not in df.columns or var_final not in df.columns:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[date_col], df[var_orig], label=f'{label_base} (original)',\n",
    "             color=colors['original'], marker='o', linestyle='none')\n",
    "    plt.plot(df[date_col], df[var_final], label=f'{label_base} (reconstructed)',\n",
    "             color=colors['final'], marker='.', linestyle='-', alpha=0.7)\n",
    "    plt.title(f\"{station_label}\")\n",
    "    plt.ylabel(f\"{label_base} ({units})\" if units else label_base)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f49b59-b98c-40f9-8355-0303ddec78d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstructed_dataframes = []\n",
    "\n",
    "for fname, df in initial_dataframes:\n",
    "    df = df.copy()\n",
    "    date_col = \"sample_date\" if \"sample_date\" in df.columns else None\n",
    "    if not date_col:\n",
    "        continue\n",
    "\n",
    "    # Station info\n",
    "    station_name = df['station_name'].dropna().unique()\n",
    "    station_id = df['station_id'].dropna().unique()\n",
    "    station_str = station_name[0] if len(station_name) > 0 else fname\n",
    "    station_id_str = f\" ({station_id[0]})\" if len(station_id) > 0 else \"\"\n",
    "    station_label = f\"{station_str}{station_id_str}\"\n",
    "\n",
    "    # Track temp columns to drop later\n",
    "    intermediate_cols = []\n",
    "\n",
    "    for var, settings in reconstruction_config.items():\n",
    "        fallback = settings.get(\"fallback\")\n",
    "        compute_from = settings.get(\"compute_from\")\n",
    "        formula = settings.get(\"formula\")\n",
    "        preserve_as = settings.get(\"preserve_as\", f\"{var}_orig\")\n",
    "        calc_temp_var = settings.get(\"calc_temp_var\", f\"{var}_calc\")\n",
    "        units = settings.get(\"units\", \"\")\n",
    "\n",
    "        if var in df.columns:\n",
    "            df[preserve_as] = df[var]\n",
    "\n",
    "        # Fill from fallback\n",
    "        if var in df.columns and fallback and fallback in df.columns:\n",
    "            df[var] = df[var].fillna(df[fallback])\n",
    "            intermediate_cols.append(fallback)\n",
    "\n",
    "        # Compute from another variable\n",
    "        if compute_from and compute_from in df.columns:\n",
    "            df[calc_temp_var] = eval(formula, {}, {compute_from: df[compute_from]})\n",
    "\n",
    "            if var in df.columns:\n",
    "                df[var] = df[var].fillna(df[calc_temp_var])\n",
    "            else:\n",
    "                df[var] = df[calc_temp_var]\n",
    "\n",
    "            intermediate_cols.extend([compute_from, calc_temp_var])\n",
    "\n",
    "        # Plot comparison\n",
    "        if preserve_as in df.columns and var in df.columns:\n",
    "            output_file = fig_dir / f\"{fname.rstrip('.nc')}_{var}_reconstruction.png\"\n",
    "            plot_reconstruction(df, date_col, preserve_as, var,\n",
    "                                label_base=var, station_label=station_label,\n",
    "                                units=units, output_path=output_file)\n",
    "\n",
    "        intermediate_cols.extend([preserve_as])\n",
    "\n",
    "    # Clean up\n",
    "    df = df.drop(columns=[col for col in intermediate_cols if col in df.columns])\n",
    "    reconstructed_dataframes.append((fname, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1e26d-1c4f-4af1-9145-79f5c8d52a58",
   "metadata": {},
   "source": [
    "## 3. Compute derived parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1fffcf-e9a1-4ee2-ab50-c44a807e6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_dataframes = []\n",
    "\n",
    "for fname, df in reconstructed_dataframes:\n",
    "    df = df.copy()\n",
    "\n",
    "    for rule in derivation_config:\n",
    "        op = rule[\"operation\"]\n",
    "        target = rule.get(\"target\")\n",
    "\n",
    "        # Scaling\n",
    "        if op == \"scale\" and target in df.columns:\n",
    "            factor = rule[\"factor\"]\n",
    "            df[target] = df[target] * factor\n",
    "\n",
    "        # Simple sum of columns\n",
    "        elif op == \"sum\":\n",
    "            sources = rule[\"sources\"]\n",
    "            if all(col in df.columns for col in sources):\n",
    "                df[target] = df[sources[0]] + df[sources[1]]\n",
    "\n",
    "        # Fill with relevant parameters  \n",
    "        elif op == \"fill_from_other\":\n",
    "            if all(col in df.columns for col in rule[\"requires\"]):\n",
    "                context = {col: df[col] for col in df.columns}\n",
    "                cond = eval(rule[\"condition\"], {}, context)\n",
    "                df.loc[cond, target] = eval(rule[\"expression\"], {}, context)\n",
    "\n",
    "        # Sum if all values are present\n",
    "        elif op == \"rowwise_sum\":\n",
    "            sources = rule[\"sources\"]\n",
    "            if all(col in df.columns for col in sources):\n",
    "                df[target] = df.apply(\n",
    "                    lambda row: sum(row[col] for col in sources)\n",
    "                    if all(pd.notna(row[col]) for col in sources)\n",
    "                    else np.nan,\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "        # Difference of two columns\n",
    "        elif op == \"difference\":\n",
    "            if all(col in df.columns for col in rule[\"requires\"]):\n",
    "                df[target] = df[rule[\"requires\"][0]] - df[rule[\"requires\"][1]]\n",
    "\n",
    "        # Apply mask before a specific date for specific files\n",
    "        elif op == \"mask_date_before\":\n",
    "            if rule[\"file_contains\"] in fname and target in df.columns and rule[\"date_column\"] in df.columns:\n",
    "                mask = pd.to_datetime(df[rule[\"date_column\"]]) < pd.Timestamp(rule[\"before\"])\n",
    "                df.loc[mask, target] = np.nan\n",
    "\n",
    "    derived_dataframes.append((fname, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd1ece7-6a4c-436f-9c59-1c1008199784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(ax, x, y, xlabel, ylabel, title, unit=None):\n",
    "    ax.scatter(x, y, alpha=0.5)\n",
    "    if pd.notna(x).any() and pd.notna(y).any():\n",
    "        lims = [min(x.min(), y.min()), max(x.max(), y.max())]\n",
    "        ax.plot(lims, lims, 'r--')\n",
    "\n",
    "    unit_str = f\" ({unit})\" if unit else \"\"\n",
    "    ax.set_xlabel(f\"{xlabel}{unit_str}\")\n",
    "    ax.set_ylabel(f\"{ylabel}{unit_str}\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_lines(ax, df, columns, labels, title, unit=None):\n",
    "    for col, label in zip(columns, labels):\n",
    "        if col in df.columns:\n",
    "            ax.plot(df['sample_date'], df[col], label=label)\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    if unit:\n",
    "        ax.set_ylabel(f'{unit}')\n",
    "    else:\n",
    "        ax.set_ylabel('##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a175670a-c72b-41d0-b86d-1483c15d9df6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fname, df in derived_dataframes:\n",
    "    station = df['station_name'].dropna().unique()\n",
    "    title = station[0] if len(station) else fname\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(14, 16))\n",
    "    fig.suptitle(f\"Station: {title}\", fontsize=16)\n",
    "\n",
    "    plotted = False\n",
    "\n",
    "    for plot in plot_config:\n",
    "        if not all(col in df.columns for col in plot[\"required\"]):\n",
    "            continue\n",
    "    \n",
    "        row, col = plot[\"subplot\"]\n",
    "        plotted = True \n",
    "    \n",
    "        if plot[\"type\"] == \"scatter\":\n",
    "            plot_scatter(\n",
    "                axs[row, col],\n",
    "                df[plot[\"x\"]],\n",
    "                df[plot[\"y\"]],\n",
    "                plot[\"xlabel\"],\n",
    "                plot[\"ylabel\"],\n",
    "                plot[\"title\"],\n",
    "                unit=plot.get(\"unit\")\n",
    "            )\n",
    "    \n",
    "        elif plot[\"type\"] == \"line\":\n",
    "            plot_lines(\n",
    "                axs[row, col],\n",
    "                df,\n",
    "                plot[\"columns\"],\n",
    "                plot[\"labels\"],\n",
    "                plot[\"title\"],\n",
    "                unit=plot.get(\"unit\")\n",
    "            )\n",
    "\n",
    "    axs[2, 1].axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    if plotted:\n",
    "        output_file = fig_dir / f\"{fname.rstrip('.nc')}_derived_pars.png\"\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51fd00-6c06-45a2-8dfa-7d3ac910df43",
   "metadata": {},
   "source": [
    "## 4. Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d77798-6475-4867-bc57-7381fb9e4c61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect_outliers_per_station(df, station_name, filename, variables=None, outlier_config=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    if outlier_config is None:\n",
    "        outlier_config = {\"lower_quantile\": 0.05, \"upper_quantile\": 0.95}\n",
    "\n",
    "    q_low = outlier_config[\"lower_quantile\"]\n",
    "    q_high = outlier_config[\"upper_quantile\"]\n",
    "\n",
    "    if variables is None:\n",
    "        variables = [\n",
    "            col for col in df.columns\n",
    "            if pd.api.types.is_numeric_dtype(df[col]) and col not in ['station_id', 'depth', 'longitude', 'latitude']\n",
    "        ]\n",
    "\n",
    "    variables = [var for var in variables if var in df.columns and not df[var].dropna().empty]\n",
    "    n_vars = len(variables)\n",
    "    if n_vars == 0:\n",
    "        return df\n",
    "\n",
    "    ncols = 2\n",
    "    nrows = math.ceil(n_vars / ncols)\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 4 * nrows), squeeze=False)\n",
    "    fig.suptitle(f\"{station_name} – Outlier Detection (Q{q_low*100:.2f}–Q{q_high*100:.2f})\", fontsize=16)\n",
    "\n",
    "    for idx, var in enumerate(variables):\n",
    "        row, col = divmod(idx, ncols)\n",
    "        ax = axs[row][col]\n",
    "\n",
    "        series = df[var].dropna()\n",
    "        p5 = series.quantile(q_low)\n",
    "        p95 = series.quantile(q_high)\n",
    "        outliers = (df[var] < p5) | (df[var] > p95)\n",
    "\n",
    "        ax.scatter(df['sample_date'], df[var], label=var, color='gray', alpha=0.7)\n",
    "        ax.scatter(df.loc[outliers, 'sample_date'], df.loc[outliers, var], color='red', label='Outliers')\n",
    "        ax.set_title(var)\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(var)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        df.loc[outliers, var] = np.nan\n",
    "\n",
    "    for i in range(n_vars, nrows * ncols):\n",
    "        row, col = divmod(i, ncols)\n",
    "        axs[row][col].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    base_name = Path(filename).stem\n",
    "    output_path = fig_dir / f\"{base_name}_detected_outliers.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "cleaned_dataframes = []\n",
    "for fname, df in derived_dataframes: \n",
    "    station_name = df['station_name'].dropna().unique()\n",
    "    station_str = station_name[0] if len(station_name) else fname\n",
    "    cleaned_df = detect_outliers_per_station(df, station_str, fname, outlier_config=outlier_config)\n",
    "    cleaned_dataframes.append((fname, cleaned_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b4dda-2e52-40c4-b8d1-a3358bef3602",
   "metadata": {},
   "source": [
    "## 5. Create datasets and assign metadata and global attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b013d7c-d4d4-471d-867d-84d38bcc1f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': 'µg/l',\n",
       " 'parameter_name': 'TOTN',\n",
       " 'long_name': 'Total Nitrogen',\n",
       " 'comment': 'TOTN is reconstructed using Eurofins data (TOTN_EF_usikker), which are considered more uncertain than NIVA data. However, due to gaps in NIVA data from 2017–2021, Eurofins measurements were included to ensure completeness.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for fname, df in cleaned_dataframes:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Make sure date is datetime\n",
    "    df[\"sample_date\"] = pd.to_datetime(df[\"sample_date\"])\n",
    "    df = df.set_index(\"sample_date\")\n",
    "\n",
    "    # Extract metadata from first row\n",
    "    lat = float(df[\"latitude\"].iloc[0])\n",
    "    lon = float(df[\"longitude\"].iloc[0])\n",
    "    station_id = int(df[\"station_id\"].iloc[0])\n",
    "    station_code = str(df[\"station_code\"].iloc[0])\n",
    "    station_name = str(df[\"station_name\"].iloc[0])\n",
    "    station_type = str(df[\"station_type\"].iloc[0])\n",
    "\n",
    "    # Remove constant fields from dataframe to avoid them being treated as varying\n",
    "    df_clean = df.drop(columns=[\"latitude\", \"longitude\", \"station_id\", \"station_code\", \"station_name\", \"station_type\"])\n",
    "\n",
    "    # Convert to xarray dataset\n",
    "    ds = xr.Dataset.from_dataframe(df_clean)\n",
    "\n",
    "    # Add attributes to sample date (time coordinate)\n",
    "    ds = ds.assign_coords(sample_date=(\"sample_date\", df.index))\n",
    "    ds[\"sample_date\"].attrs.update({\"standard_name\": \"time\", \"long_name\": \"Time of measurement\", \"axis\": \"T\"})\n",
    "\n",
    "    # Add scalar coordinates\n",
    "    ds = ds.assign_coords(\n",
    "        latitude=xr.DataArray(lat, dims=(), attrs={\"standard_name\": \"latitude\", \"long_name\": \"latitude\", \"units\": \"degree_north\"}),\n",
    "        longitude=xr.DataArray(lon, dims=(), attrs={\"standard_name\": \"longitude\", \"long_name\": \"longitude\", \"units\": \"degree_east\"})\n",
    "    )\n",
    "    ds = ds.set_coords([\"latitude\", \"longitude\"])\n",
    "\n",
    "    # Add station info \n",
    "    ds[\"station_id\"] = xr.DataArray(station_id, dims=())\n",
    "    ds[\"station_code\"] = xr.DataArray(station_code, dims=())\n",
    "    ds[\"station_name\"] = xr.DataArray(station_name, dims=(), attrs={\"cf_role\": \"timeseries_id\"})\n",
    "    ds[\"station_type\"] = xr.DataArray(station_type, dims=())\n",
    "\n",
    "    # Add metadata to data variables\n",
    "    for var in ds.data_vars:\n",
    "        if var in [\"station_id\", \"station_code\", \"station_name\", \"station_type\"]:\n",
    "            continue  # skip scalar metadata fields\n",
    "    \n",
    "        match = pars_metadata_df[pars_metadata_df[\"parameter_name\"] == var]\n",
    "    \n",
    "        if not match.empty:\n",
    "            row = match.iloc[0]\n",
    "            ds[var].attrs[\"units\"] = row[\"unit\"]\n",
    "            ds[var].attrs[\"parameter_name\"] = row[\"parameter_name\"]\n",
    "            ds[var].attrs[\"long_name\"] = standard_name_map.get(var, row[\"parameter_name\"])\n",
    "    \n",
    "            # Assign comments \n",
    "            if var in var_comments:\n",
    "                ds[var].attrs[\"comment\"] = var_comments[var]\n",
    "                       \n",
    "    datasets.append((fname, ds))\n",
    "\n",
    "# Metadata example\n",
    "datasets[0][1][\"TOTN\"].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05173c10-195d-4322-b0d9-ae9f3e5675ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved NetCDF: ..\\..\\data\\river\\water_chemistry\\cleaned\\cleaned_riverchem_40352_23-06-2025.nc\n",
      "Saved NetCDF: ..\\..\\data\\river\\water_chemistry\\cleaned\\cleaned_riverchem_40355_23-06-2025.nc\n",
      "Saved NetCDF: ..\\..\\data\\river\\water_chemistry\\cleaned\\cleaned_riverchem_40356_23-06-2025.nc\n"
     ]
    }
   ],
   "source": [
    "int_encoding = {\"dtype\": \"int32\", \"_FillValue\": -9999}\n",
    "\n",
    "for fname, ds in datasets:\n",
    "    # Extract metadata\n",
    "    station_name = str(ds[\"station_name\"].values.item())\n",
    "    station_code = str(ds[\"station_code\"].values.item())\n",
    "    station_id = int(ds[\"station_id\"].values.item())\n",
    "    lat = float(ds[\"latitude\"].values.item())\n",
    "    lon = float(ds[\"longitude\"].values.item())\n",
    "\n",
    "    unique_id = str(uuid.uuid5(processed_namespace_uuid, str(station_id)))\n",
    "\n",
    "    # Prepare dataset-specific attributes\n",
    "    dataset_metadata = global_metadata_config.copy()\n",
    "    dataset_metadata.update({\n",
    "        \"id\": unique_id,\n",
    "        \"title\": f\"Cleaned water chemistry measurements at station {station_name}\",\n",
    "        \"title_no\": f\"Rensede kjemiske målinger ved stasjon {station_name}\",\n",
    "        \"summary\": f\"Cleaned long-term water chemistry monitoring at station {station_name} (code: {station_code})\",\n",
    "        \"summary_no\": f\"Rensede, langsiktige vannkjemiske målinger ved stasjon {station_name} (kode: {station_code})\",\n",
    "        \"date_created\": datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"time_coverage_start\": np.datetime_as_string(ds.sample_date.min().values, unit=\"s\", timezone=\"UTC\"),\n",
    "        \"time_coverage_end\": np.datetime_as_string(ds.sample_date.max().values, unit=\"s\", timezone=\"UTC\"),\n",
    "        \"geospatial_lat_min\": lat,\n",
    "        \"geospatial_lat_max\": lat,\n",
    "        \"geospatial_lon_min\": lon,\n",
    "        \"geospatial_lon_max\": lon,\n",
    "    })\n",
    "\n",
    "    # Assign attributes\n",
    "    ds.attrs = dataset_metadata\n",
    "\n",
    "    # Define encoding\n",
    "    encoding = {\n",
    "        \"sample_date\": {\n",
    "            \"dtype\": \"int32\",\n",
    "            \"_FillValue\": None,\n",
    "            \"units\": \"seconds since 1970-01-01 00:00:00\",\n",
    "        },\n",
    "        \"longitude\": {\"_FillValue\": None},\n",
    "        \"latitude\": {\"_FillValue\": None},\n",
    "    }\n",
    "    if \"station_id\" in ds:\n",
    "        encoding[\"station_id\"] = int_encoding\n",
    "\n",
    "    # Save file\n",
    "    output_path = output_dir / f\"cleaned_riverchem_{station_id}_{datetime.now().strftime('%d-%m-%Y')}.nc\"\n",
    "    ds.to_netcdf(output_path, encoding=encoding)\n",
    "    print(f\"Saved NetCDF: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b359b-4c5b-46d5-ab84-ac3f1285ff86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
